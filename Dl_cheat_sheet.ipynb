{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 2: Logistic regression / MLPs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Logistic regression - Cross entropy loss </h5>\n",
    "memory complexity of eqn : O(N * d^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss( y_true, y_pred ):\n",
    "\n",
    "    epsilon = 1e-15  # small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip predictions to avoid log(0)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss = - np.sum(y_true * np.log(y_pred))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Multi Layer Perceptron</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.1, 0.2],\n",
      "       [0.3, 0.4],\n",
      "       [0.5, 0.6],\n",
      "       [0.7, 0.8]]), array([[0.2, 0.3, 0.4, 0.5]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (4,2) not aligned: 2 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m    131\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m],[ \u001b[39m2\u001b[39m]])\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 132\u001b[0m y \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    133\u001b[0m \u001b[39mprint\u001b[39m(y)\n\u001b[1;32m    135\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m a \u001b[39m=\u001b[39m x\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(a, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[i]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases[i]\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_values\u001b[39m.\u001b[39mappend(z)\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_layers \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,2) and (4,2) not aligned: 2 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu'):\n",
    "        self.num_layers = 1 + len(hidden_sizes) + 1  # Input layer + hidden layers + output layer\n",
    "        self.sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.weights = [None] * (self.num_layers - 1)\n",
    "        self.biases = [None] * (self.num_layers - 1)\n",
    "        self.set_activation(activation)\n",
    "        \n",
    "    def set_weights(self, layer_idx, weights):\n",
    "        self.weights[layer_idx] = weights\n",
    "        \n",
    "    def set_biases(self, layer_idx, biases):\n",
    "        self.biases[layer_idx] = biases\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        self.activation_name = activation\n",
    "        if activation == 'prelu':\n",
    "            self.activation = self.prelu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = self.gelu\n",
    "        elif activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.tanh\n",
    "        elif activation == 'linear':\n",
    "            self.activation = self.linear\n",
    "        elif activation == 'elu':\n",
    "            self.activation = self.elu\n",
    "        elif activation == 'swish':\n",
    "            self.activation = self.swish\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{activation}' not supported.\")\n",
    "    \n",
    "    def prelu(self, x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    def elu(self, x, alpha=1.0):\n",
    "        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "    \n",
    "    def swish(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "     # ... (other activation functions and their derivatives)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = [x]\n",
    "        self.z_values = []\n",
    "        a = x\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            if i in self.skip_layers and i > 0:\n",
    "                a = self.activation(z) + self.activations[i - 1]  # Adding skip connection\n",
    "            else:\n",
    "                a = self.activation(z)\n",
    "            \n",
    "            self.activations.append(a)\n",
    "        \n",
    "        return a\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        output = self.activations[-1]\n",
    "        loss = np.mean((output - y)**2)\n",
    "        \n",
    "        delta = (output - y) * self.activation_derivative(output)\n",
    "        self.deltas = [delta]\n",
    "        \n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.activations[i])\n",
    "            self.deltas.insert(0, delta)\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights[i] -= learning_rate * np.dot(self.activations[i].T, self.deltas[i])\n",
    "            self.biases[i] -= learning_rate * self.deltas[i]\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation_name == 'prelu':\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        elif self.activation_name == 'gelu':\n",
    "            return 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "        elif self.activation_name == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.activation_name == 'tanh':\n",
    "            return 1 - np.square(np.tanh(x))\n",
    "        elif self.activation_name == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        elif self.activation_name == 'elu':\n",
    "            return np.where(x > 0, 1, self.activation(x) + 1)\n",
    "        elif self.activation_name == 'swish':\n",
    "            return self.sigmoid(x) * (1 + x * (1 - self.sigmoid(x)))\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{self.activation_name}' not supported.\")\n",
    "\n",
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_sizes = [4]\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "activation_function = 'relu'\n",
    "\n",
    "mlp = MLP(input_size, hidden_sizes, output_size, activation_function)\n",
    "\n",
    "# Set custom weights and biases for each layer\n",
    "custom_weights = [np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]]),\n",
    "                  np.array([[0.2, 0.3, 0.4, 0.5]])]\n",
    "custom_biases = [np.array([[0.1, 0.2, 0.3, 0.4]]),\n",
    "                 np.array([[0.2]])]\n",
    "\n",
    "for i in range(len(custom_weights)):\n",
    "    mlp.set_weights(i, custom_weights[i])\n",
    "    mlp.set_biases(i, custom_biases[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:yellow;\">Attention Calculator</span> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[1 1]\n",
      " [0 0]\n",
      " [1 1]]\n",
      "\n",
      "Query:\n",
      "[[2.]\n",
      " [0.]\n",
      " [2.]]\n",
      "\n",
      "Key:\n",
      "[[2.]\n",
      " [0.]\n",
      " [2.]]\n",
      "\n",
      "Value:\n",
      "[[1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "Attention Scores:\n",
      "[[4. 0. 4.]\n",
      " [0. 0. 0.]\n",
      " [4. 0. 4.]]\n",
      "\n",
      "Attention Weights:\n",
      "[[0.5  0.01 0.5 ]\n",
      " [0.33 0.33 0.33]\n",
      " [0.5  0.01 0.5 ]]\n",
      "\n",
      "Context Vector:\n",
      "[[0.99]\n",
      " [0.67]\n",
      " [0.99]]\n",
      "\n",
      "Output:\n",
      "[[0.5  0.5 ]\n",
      " [0.33 0.33]\n",
      " [0.5  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given values (example values)\n",
    "wq = np.array([[0.0], [2.0]])\n",
    "wk = np.array([[2.0], [0.0]])\n",
    "wv = np.array([[0.5], [0.5]])\n",
    "wo = np.array([[0.5, 0.5]])\n",
    "\n",
    "x = np.array([[1, 1], [0, 0], [1, 1]])\n",
    "\n",
    "# Calculating self-attention\n",
    "q = np.dot(x, wq)  # Query\n",
    "k = np.dot(x, wk)  # Key\n",
    "v = np.dot(x, wv)  # Value\n",
    "\n",
    "# Compute attention scores (unnormalized)\n",
    "attention_scores = np.dot(q, k.T)  # Shape: (sequence_length, sequence_length)\n",
    "attention_scores /= np.sqrt(wq.shape[1])  # Normalizing by the square root of the dimension\n",
    "\n",
    "# Apply softmax to get attention weights\n",
    "attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the context vector using attention weights and values\n",
    "context_vector = np.dot(attention_weights, v)\n",
    "\n",
    "# Calculate the final output using the output weights\n",
    "output = np.dot(context_vector, wo)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nQuery:\")\n",
    "print(q)\n",
    "print(\"\\nKey:\")\n",
    "print(k)\n",
    "print(\"\\nValue:\")\n",
    "print(v)\n",
    "print(\"\\nAttention Scores:\")\n",
    "print(np.round(attention_scores,2))\n",
    "print(\"\\nAttention Weights:\")\n",
    "print( np.round(attention_weights,2))\n",
    "print(\"\\nContext Vector:\")\n",
    "print(np.round(context_vector,2))\n",
    "print(\"\\nOutput:\")\n",
    "print(np.round(output,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Number Calculator </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:yellow;\">MLP params</span> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "def mlp_params(layer_sizes):\n",
    "    params = 0\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        params += (layer_sizes[i] * layer_sizes[i+1]) + layer_sizes[i+1]\n",
    "    return params\n",
    "\n",
    "layer_sizes = [ 5, 20]\n",
    "print(mlp_params(layer_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <span style=\"color:yellow;\">Batch Norm params</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for CNN : 20\n"
     ]
    }
   ],
   "source": [
    "def batch_norm_params(input_dimension, network_type):\n",
    "    # Function to calculate number of trainable parameters in batch normalization layer\n",
    "    # Input: input_dimension: number of input channels\n",
    "    #        network_type: CNN or Fully Connected\n",
    "    # Output: number of trainable parameters in batch normalization layer\n",
    "    if network_type == 'CNN':\n",
    "        return 2*input_dimension\n",
    "    elif network_type == 'FC':\n",
    "        return 2*input_dimension\n",
    "\n",
    "input_dimension = 10 # Number of input channels to bn layer\n",
    "network_type = 'CNN'\n",
    "print(\"Number of trainable parameters for\", network_type, \":\", batch_norm_params(input_dimension, network_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <span style=\"color:yellow;\">Layer Norm params</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for CNN : 180\n"
     ]
    }
   ],
   "source": [
    "def layer_norm_params(input_dimension, spatial_dimension=None, network_type='FC'):\n",
    "    \"\"\"\n",
    "    Function to calculate number of trainable parameters in layer normalization layer\n",
    "    \n",
    "    Inputs:\n",
    "    - input_dimension: number of input channels or neurons\n",
    "    - spatial_dimension: tuple (height, width) for spatial dimensions in CNN. Not required for FC.\n",
    "    - network_type: 'CNN' or 'FC'\n",
    "    \n",
    "    Output: number of trainable parameters in layer normalization layer\n",
    "    \"\"\"\n",
    "    \n",
    "    if network_type == 'CNN':\n",
    "        if spatial_dimension is None:\n",
    "            raise ValueError(\"For CNN, spatial_dimension (height, width) must be provided\")\n",
    "        height, width = spatial_dimension\n",
    "        return 2 * input_dimension * height * width\n",
    "    elif network_type == 'FC':\n",
    "        return 2 * input_dimension\n",
    "\n",
    "input_dimension = 10  # Number of input channels to ln layer\n",
    "spatial_dimension = (3, 3)  # Assuming a spatial size of 32x32 for CNN\n",
    "network_type = 'CNN'\n",
    "print(\"Number of trainable parameters for\", network_type, \":\", layer_norm_params(input_dimension, spatial_dimension, network_type))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <span style=\"color:yellow;\">Recurrent network params</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters for LSTM : 328704\n"
     ]
    }
   ],
   "source": [
    "def rnn_params(input_dim, hidden_dim, cell_type='RNN'):\n",
    "    \"\"\"\n",
    "    Function to calculate number of trainable parameters in RNN, LSTM, or GRU cell\n",
    "    \n",
    "    Inputs:\n",
    "    - input_dim: Dimensionality of input data\n",
    "    - hidden_dim: Dimensionality of hidden state\n",
    "    - cell_type: 'RNN', 'LSTM', or 'GRU'\n",
    "    \n",
    "    Output: number of trainable parameters in the specified cell\n",
    "    \"\"\"\n",
    "    \n",
    "    if cell_type == 'RNN':\n",
    "        return input_dim * hidden_dim + hidden_dim**2 + hidden_dim\n",
    "    elif cell_type == 'LSTM':\n",
    "        return 4 * (input_dim * hidden_dim + hidden_dim**2 + hidden_dim)\n",
    "    elif cell_type == 'GRU':\n",
    "        return 3 * (input_dim * hidden_dim + hidden_dim**2 + hidden_dim)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid cell_type. Choose from 'RNN', 'LSTM', or 'GRU'.\")\n",
    "\n",
    "input_dim = 64 # Example input dimension\n",
    "hidden_dim = 256 # Example hidden dimension\n",
    "cell_type = 'LSTM'\n",
    "print(\"Number of trainable parameters for\", cell_type, \":\", rnn_params(input_dim, hidden_dim, cell_type))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m params_A, params_B\n\u001b[1;32m     25\u001b[0m \u001b[39m# Test functions\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mprint\u001b[39m(maintain_rf([\u001b[39m5\u001b[39;49m, \u001b[39m7\u001b[39;49m], \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, [\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m], \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))  \u001b[39m# Example: Replacing 5x7 filter with 3x1 filters\u001b[39;00m\n\u001b[1;32m     27\u001b[0m input_channels \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     28\u001b[0m output_channels \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mmaintain_rf\u001b[0;34m(filter_A_dims, stride_A, dilation_A, filter_B_dims, stride_B, dilation_B)\u001b[0m\n\u001b[1;32m     12\u001b[0m RF_B \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m RF_B \u001b[39m<\u001b[39m RF_A:\n\u001b[0;32m---> 14\u001b[0m     RF_B \u001b[39m=\u001b[39m ((dim_B \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39m dilation_B) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (RF_B \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m stride_B\n\u001b[1;32m     15\u001b[0m     n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     16\u001b[0m n_filters\u001b[39m.\u001b[39mappend(n)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def maintain_rf(filter_A_dims, stride_A, dilation_A, filter_B_dims, stride_B, dilation_B):\n",
    "    # Ensure filter dimensions are of the same length for A and B\n",
    "    assert len(filter_A_dims) == len(filter_B_dims), \"Dimension mismatch\"\n",
    "    \n",
    "    n_filters = []\n",
    "    \n",
    "    for dim_A, dim_B in zip(filter_A_dims, filter_B_dims):\n",
    "        RF_A = ((dim_A - 1) * dilation_A) + 1\n",
    "        \n",
    "        n = 0\n",
    "        RF_B = 1\n",
    "        while RF_B < RF_A:\n",
    "            RF_B = ((dim_B - 1) * dilation_B) + 1 + (RF_B - 1) * stride_B\n",
    "            n += 1\n",
    "        n_filters.append(n)\n",
    "\n",
    "    return n_filters\n",
    "\n",
    "def compare_parameters(input_channels, output_channels, filter_A_dims, filter_B_dims, n_filters):\n",
    "    params_A = input_channels * output_channels * filter_A_dims[0] * filter_A_dims[1]\n",
    "    params_B = input_channels * output_channels * filter_B_dims[0] * filter_B_dims[1] * n_filters[0] * n_filters[1]\n",
    "    return params_A, params_B\n",
    "\n",
    "# Test functions\n",
    "print(maintain_rf([5, 7], 1, 1, [3, 1], 1, 1))  # Example: Replacing 5x7 filter with 3x1 filters\n",
    "input_channels = 3\n",
    "output_channels = 64\n",
    "filter_A_dims = [5, 7]\n",
    "filter_B_dims = [3, 1]\n",
    "n_filters = maintain_rf(filter_A_dims, 1, 1, filter_B_dims, 1, 1)\n",
    "print(compare_parameters(input_channels, output_channels, filter_A_dims, filter_B_dims, n_filters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
